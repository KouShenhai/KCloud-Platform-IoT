<?xml version="1.0" encoding="UTF-8"?>
<!--
 /*
 * Copyright (c) 2022-2024 KCloud-Platform-IoT Author or Authors. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
-->
<!--
    OFF > FATAL > ERROR > WARN > INFO > DEBUG > TRACE > ALL
    monitorInterval: 间隔秒数，自动检测配置文件的变更和重新配置本身
 -->
<Configuration status="ERROR" monitorInterval="5">
  <!-- 变量配置 -->
  <Properties>
    <!-- 存放目录 -->
    <property name="LOG_PATH" value="/home/ubuntu/auth"/>
    <!-- 日志文件大小 -->
    <property name="LOG_ROLL_SIZE" value="1 GB"/>
    <!-- 服务名称 -->
    <property name="SERVICE_ID" value="laokou-auth"/>
    <!-- -Dlog4j.skipJansi=false -->
    <!-- 日志格式 -->
    <!--
			%d 日期
			%t 线程

			对于系统参数，使用${sys:xxx}即可，例如读取-Dserver.port=1111，即${sys:server.port}
			spanId  => %X{spanId}
			traceId => %X{traceId}
    -->
    <property name="LOG_CONSOLE_PATTERN"
              value="%d{yyyy-MM-dd HH:mm:ss.SSS} ---> [${SERVICE_ID} - ${sys:ip}:${sys:server.port}] - [%X{traceId}] - [%X{spanId}] - [ %style{%-5level}{red}] - [%t] - %logger : %msg%n"/>
    <property name="LOG_FILE_PATTERN"
              value="%d{yyyy-MM-dd HH:mm:ss.SSS} ---> [${SERVICE_ID} - ${sys:ip}:${sys:server.port}] - [%X{traceId}] - [%X{spanId}] - [ %-5level] - [%t] - %logger : %msg%n"/>
    <!-- 日志保留30天 -->
    <property name="LOG_DAYS" value="30"/>
    <!-- 1天滚动一次 -->
    <property name="LOG_INTERVAL" value="1"/>
  </Properties>
  <Appenders>
    <!-- 控制台输出 -->
    <Console name="console" target="SYSTEM_OUT">
      <PatternLayout pattern="${LOG_CONSOLE_PATTERN}"/>
    </Console>
    <RollingRandomAccessFile name="file" fileName="${LOG_PATH}/${SERVICE_ID}.log"
                             filePattern="${LOG_PATH}/%d{yyyyMMdd}/${SERVICE_ID}_%d{yyyy-MM-dd}.%i.log.gz"
                             immediateFlush="false">
      <Filters>
        <ThresholdFilter level="ERROR" onMatch="ACCEPT" onMismatch="DENY"/>
      </Filters>
      <PatternLayout pattern="${LOG_FILE_PATTERN}"/>
      <Policies>
        <TimeBasedTriggeringPolicy interval="${LOG_INTERVAL}"/>
        <SizeBasedTriggeringPolicy size="${LOG_ROLL_SIZE}"/>
      </Policies>
      <DefaultRolloverStrategy max="${LOG_DAYS}"/>
    </RollingRandomAccessFile>
    <RollingRandomAccessFile name="failover_kafka" fileName="${LOG_PATH}/${SERVICE_ID}_kafka.log"
                             filePattern="${LOG_PATH}/%d{yyyyMMdd}/${SERVICE_ID}_kafka_%d{yyyy-MM-dd}.%i.log.gz"
                             immediateFlush="false">
      <Filters>
        <ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY"/>
      </Filters>
      <PatternLayout pattern="${LOG_FILE_PATTERN}"/>
      <Policies>
        <TimeBasedTriggeringPolicy interval="${LOG_INTERVAL}"/>
        <SizeBasedTriggeringPolicy size="${LOG_ROLL_SIZE}"/>
      </Policies>
      <DefaultRolloverStrategy max="${LOG_DAYS}"/>
    </RollingRandomAccessFile>
    <Failover name="failover" primary="kafka" retryIntervalSeconds="600">
      <Failovers>
        <AppenderRef ref="failover_kafka"/>
      </Failovers>
    </Failover>
    <Async name="async_file" bufferSize="2000" blocking="false">
      <AppenderRef ref="file"/>
    </Async>
    <Async name="async_kafka" bufferSize="2000" blocking="false">
      <AppenderRef ref="failover"/>
    </Async>
    <Kafka name="kafka" topic="laokou_trace_topic" ignoreExceptions="false">
      <PatternLayout>
        <!--

           注意：真实的生产环境，日志打印的内容是五花八门，日志内容会出现一些莫名其他的特殊符号，导致json无法反序列化
           因此，可以利用Pattern Layout 提供的标签enc，enc支持4种转义，HTML/XML/JSON/CRLF，默认进行HTML转义
          目前，只对JSON处理，即%enc{%m}{JSON} => {"msg":"%enc{%m}{JSON}"}

        -->
        <pattern>
          {
          "serviceId":"${SERVICE_ID}",
          "profile":"prod",
          "dateTime":"%d{yyyy-MM-dd HH:mm:ss.SSS}",
          "traceId":"%X{traceId}",
          "spanId":"%X{spanId}",
          "address":"${sys:ip}:${sys:server.port}",
          "level":"%-5level",
          "thread":"%thread",
          "logger":"%logger",
          "msg":"%enc{%m}{JSON}"
          }
        </pattern>
      </PatternLayout>
      <!-- 生产者发送消息最大阻塞时间，单位为毫秒，生产者阻塞超过2秒，则抛出异常并中断发送【生产者内部缓冲区已满或元数据不可用，send()会阻塞等待】 -->
      <Property name="max.block.ms">2000</Property>
      <!-- 客户端发送请求到kafka broker超时时间，单位为毫秒，2秒内没有从kafka broker收到响应，则认为请求失败则抛出异常 -->
      <Property name="request.timeout.ms">2000</Property>
      <Property name="bootstrap.servers">kafka:9092</Property>
    </Kafka>
  </Appenders>
  <!--
   additivity      => 需不需要打印此logger继承的父logger，false只打印当前logger，true继续打印上一层logger，直至root
   includeLocation => 显示文件行数，方法名等信息，true显示，false不显示，可以减少日志输出的体积，加快日志写入速度
 -->
  <Loggers>
    <AsyncLogger name="org.laokou" additivity="FALSE" includeLocation="FALSE" level="INFO">
      <AppenderRef ref="async_kafka"/>
    </AsyncLogger>
    <AsyncRoot level="ERROR" additivity="FALSE" includeLocation="FALSE">
      <AppenderRef ref="console"/>
      <AppenderRef ref="async_file"/>
    </AsyncRoot>
  </Loggers>
</Configuration>
